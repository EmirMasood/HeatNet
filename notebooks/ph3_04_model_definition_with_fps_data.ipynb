{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22413,"status":"ok","timestamp":1748507638342,"user":{"displayName":"AAEA","userId":"14660609484008760889"},"user_tz":-120},"id":"NgVRZUfuzQay","outputId":"4ede034a-3ecf-4739-8903-c9aaf9be56b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"DDorg2pA7JUZ","executionInfo":{"status":"ok","timestamp":1748507663397,"user_tz":-120,"elapsed":9265,"user":{"displayName":"AAEA","userId":"14660609484008760889"}}},"outputs":[],"source":["import torchvision.models as models\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import os\n","import random\n","import json\n","from collections import defaultdict\n","from torch.utils.data import Dataset\n","from torchvision import transforms\n","from PIL import Image\n","from torch.utils.data import DataLoader\n","from torch.utils.data import random_split\n","from torch.optim.lr_scheduler import ReduceLROnPlateau"]},{"cell_type":"markdown","metadata":{"id":"Vyo_0p1e--96"},"source":["## Creating ResNet network and modifying it"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"gnEJI2t-cRC6","executionInfo":{"status":"ok","timestamp":1748507663400,"user_tz":-120,"elapsed":5,"user":{"displayName":"AAEA","userId":"14660609484008760889"}}},"outputs":[],"source":["class KeypointHeatmapDataset(Dataset):\n","    def __init__(self, img_dir, hm_dir, img_ext=\".png\", hm_ext=\".pt\"):\n","        self.img_dir = img_dir\n","        self.hm_dir  = hm_dir\n","        self.img_ext = img_ext\n","        self.hm_ext = hm_ext\n","\n","        all_names = sorted([\n","            os.path.splitext(f)[0]\n","            for f in os.listdir(self.img_dir)\n","            if f.endswith(self.img_ext)\n","        ])\n","\n","        #splitting the data based on the first two numbers\n","        groups = defaultdict(list)\n","        for name in all_names:\n","            obj_id = name.split(\"_\")[0]\n","            groups[obj_id].append(name)\n","\n","        total     = len(all_names)\n","        ratio     = 1\n","        target_n  = int(total * ratio)\n","\n","        floors, remainders = {}, {}\n","        for obj_id, lst in groups.items():\n","            raw        = len(lst) * ratio\n","            fl         = int(raw)\n","            floors[obj_id]    = fl\n","            remainders[obj_id] = raw - fl\n","\n","        sum_floor = sum(floors.values())\n","        leftover  = target_n - sum_floor\n","        for obj_id in sorted(remainders, key=lambda x: remainders[x], reverse=True)[:leftover]:\n","            floors[obj_id] += 1\n","\n","        chosen = []\n","        random.seed(42)\n","        for obj_id, lst in groups.items():\n","            k = floors[obj_id]\n","            chosen.extend(random.sample(lst, k))\n","\n","        self.basenames = sorted(chosen)\n","\n","      #   self.basenames = [\n","      #   os.path.splitext(f)[0]\n","      #   for f in os.listdir(self.img_dir)\n","      #   if f.endswith(self.img_ext)\n","      # ]\n","\n","        self.transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            # normalizing the pictures\n","            transforms.Normalize(\n","                mean=[0.485,0.456,0.406],\n","                std =[0.229,0.224,0.225]\n","            ),\n","        ])\n","\n","        img_basenames = {\n","          os.path.splitext(f)[0]\n","          for f in os.listdir(self.img_dir)\n","          if f.endswith(self.img_ext)\n","        }\n","        hm_basenames = {\n","            os.path.splitext(f)[0]\n","            for f in os.listdir(self.hm_dir)\n","            if f.endswith(self.hm_ext)\n","        }\n","        missing_in_hm = img_basenames - hm_basenames\n","        missing_in_img = hm_basenames - img_basenames\n","        print(\"Missing heatmaps for images:\", missing_in_hm)\n","        print(\"Missing images for heatmaps:\", missing_in_img)\n","\n","\n","    def __len__(self):\n","        return len(self.basenames)\n","\n","    def __getitem__(self, idx):\n","        name = self.basenames[idx]\n","        img_path = os.path.join(self.img_dir, name + self.img_ext)\n","        img = Image.open(img_path).convert(\"RGB\")\n","        img = self.transform(img)        # [3,256,256] tensor/image with 3 colors and dimensions 256x256\n","        hm_path = os.path.join(self.hm_dir, name + self.hm_ext)\n","        heatmaps = torch.load(hm_path)   # [50,64,64], 50 heatmaps with dimensions 64x64\n","\n","        return img, heatmaps\n","\n","# this makes the inputs for resnet network in the right format\n"]},{"cell_type":"markdown","metadata":{"id":"V_0yBMMvyUpF"},"source":["Definition of the DataLoader"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"yFmx4ibsyXdV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748518611091,"user_tz":-120,"elapsed":562,"user":{"displayName":"AAEA","userId":"14660609484008760889"}},"outputId":"95e8550b-064b-4b86-c593-e55a61191b20"},"outputs":[{"output_type":"stream","name":"stdout","text":["Missing heatmaps for images: set()\n","Missing images for heatmaps: {'05_0724', '08_0717', '02_0680', '05_0299', '15_0029', '02_0647'}\n"]}],"source":["img_folder = \"/content/drive/MyDrive/MLDL/6D-Pose-Estimation/data/cropped_resized_data\"\n","hm_folder  = \"/content/drive/MyDrive/MLDL/6D-Pose-Estimation/data/point_sampling_data/heatmaps_sigma_2/heatmaps_fps\"\n","\n","dataset = KeypointHeatmapDataset(\n","    img_dir=img_folder,\n","    hm_dir=hm_folder,\n","    img_ext=\".png\",\n","    hm_ext=\".pt\"\n",")\n","\n","loader = DataLoader(\n","  dataset,\n","  batch_size=16,\n","  shuffle=True,\n","  num_workers=2\n",")"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13391,"status":"ok","timestamp":1748517549094,"user":{"displayName":"AAEA","userId":"14660609484008760889"},"user_tz":-120},"id":"keOvsqD9ztX3","outputId":"59e4456a-1e0b-449b-da68-aeab879b77c3"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([16, 3, 256, 256])\n","torch.Size([16, 50, 64, 64])\n"]}],"source":["#checking\n","imgs, hms = next(iter(loader))\n","print(imgs.shape) # torch.Size([16, 3, 256, 256])\n","print(hms.shape) # torch.Size([16, 50, 64, 64])"]},{"cell_type":"markdown","metadata":{"id":"_kao7ZMc1ms4"},"source":["Taking pretrained ResNet-101, remove the final pooling and fully-connected layers, add new head of layers and give output that has format [batch_size, num_keypoints, 64, 64]"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"FFdUqOyG2ObI","executionInfo":{"status":"ok","timestamp":1748518709113,"user_tz":-120,"elapsed":41,"user":{"displayName":"AAEA","userId":"14660609484008760889"}}},"outputs":[],"source":["class HeatmapHead(nn.Module):\n","    def __init__(self, num_keypoints, in_channels):\n","        super().__init__()\n","        # three convolutional layers: 8×8 → 16×16 → 32×32 → 64×64\n","        #resnet initially makes the image smaller by 32 so in our case the picture becomes 8x8\n","        self.deconv = nn.Sequential(\n","            # 1st deconv: 2048→256 channels, doubles spatial size (8→16)\n","            nn.ConvTranspose2d(in_channels, 256, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","\n","            # 2nd deconv: 256→256 channels, doubles spatial size (16→32)\n","            nn.ConvTranspose2d(256, 256, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","\n","            # 3rd deconv: 256→256 channels, doubles spatial size (32→64)\n","            nn.ConvTranspose2d(256, 256, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","        )\n","        # Final 1×1 conv: 256→num_keypoints channels, keeps spatial size 64×64\n","        self.final = nn.Conv2d(256, num_keypoints, kernel_size=1)\n","\n","    def forward(self, x):\n","        x = self.deconv(x)  #[B,256,64,64]\n","        x = self.final(x)   #[B,num_keypoints,64,64]\n","        return x"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"lw6F9LKv9IXs","executionInfo":{"status":"ok","timestamp":1748518713162,"user_tz":-120,"elapsed":2,"user":{"displayName":"AAEA","userId":"14660609484008760889"}}},"outputs":[],"source":["class KeypointHeatmapNet(nn.Module):\n","    def __init__(self, num_keypoints=50):\n","        super().__init__()\n","        #loading the pretrained ResNet-50 and striping off last pool+fc\n","        backbone = models.resnet18(pretrained=True)\n","        self.backbone = nn.Sequential(*list(backbone.children())[:-2])\n","\n","        #attach the layers we want to add\n","        self.head = HeatmapHead(num_keypoints, in_channels=512)\n","\n","    def forward(self, x):\n","        # x: [B, 3, 256, 256], B is batch size\n","        feat = self.backbone(x)     # feat: [B, 2048, 8, 8]\n","        heatmaps = self.head(feat)  # heatmaps: [B, 50, 64, 64]\n","        return heatmaps"]},{"cell_type":"markdown","metadata":{"id":"ns2x3nF0_PH1"},"source":["## Defining the model, loss function and optimizer"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"1z50t2bw_TEE","executionInfo":{"status":"ok","timestamp":1748518715449,"user_tz":-120,"elapsed":39,"user":{"displayName":"AAEA","userId":"14660609484008760889"}}},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":81,"status":"ok","timestamp":1748518716184,"user":{"displayName":"AAEA","userId":"14660609484008760889"},"user_tz":-120},"id":"UuPwypmrdDGE","outputId":"3a9f8901-11bc-4f58-977a-d15cedba4264"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","GPU name: Tesla T4\n"]}],"source":["#to check if we are using cuda\n","print(\"Using device:\", device)\n","if device.type == \"cuda\":\n","    print(\"GPU name:\", torch.cuda.get_device_name(0))"]},{"cell_type":"markdown","metadata":{"id":"azLt-qKaLrhT"},"source":["Spliting the data"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"TVE5UiH-Lvco","executionInfo":{"status":"ok","timestamp":1748518719607,"user_tz":-120,"elapsed":2,"user":{"displayName":"AAEA","userId":"14660609484008760889"}}},"outputs":[],"source":["n_total = len(dataset)\n","n_train = int(0.8 * n_total)\n","n_val = n_total - n_train\n","\n","train_ds, val_ds = random_split(\n","  dataset,\n","  [n_train, n_val],\n","  generator=torch.Generator().manual_seed(8)\n",")"]},{"cell_type":"markdown","metadata":{"id":"H-QwJTiYMd9k"},"source":["Instance the DataLoaders, model, loss function and optimizers"]},{"cell_type":"code","source":["#definition of a loss function\n","import torch.nn.functional as F\n","\n","def focal_heatmap_loss(pred, gt, alpha=2.0, gamma=4.0, eps=1e-6):\n","    \"\"\"\n","    pred: raw network output, shape [B,K,H,W]\n","    gt:   ground-truth heatmaps in [0..1], same shape [B,K,H,W]\n","    \"\"\"\n","    # 1) pretvori output u “confidence” 0..1\n","    p = torch.sigmoid(pred)\n","\n","    # 2) positive term: fokus na gt==1 region\n","    pos = - alpha * (1 - p)**gamma * gt * torch.log(p + eps)\n","\n","    # 3) negative term: background\n","    neg = - (1 - gt) * torch.log(1 - p + eps)\n","\n","    return (pos + neg).mean()"],"metadata":{"id":"lPOPyDoZgPcb","executionInfo":{"status":"ok","timestamp":1748518722898,"user_tz":-120,"elapsed":2,"user":{"displayName":"AAEA","userId":"14660609484008760889"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["import os\n","os.cpu_count()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SE8oPXnor_0r","executionInfo":{"status":"ok","timestamp":1748518103795,"user_tz":-120,"elapsed":43,"user":{"displayName":"AAEA","userId":"14660609484008760889"}},"outputId":"0aba6fdd-c71a-414b-b8de-111efe838228"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","execution_count":43,"metadata":{"id":"QBTMMvZgMkWd","executionInfo":{"status":"ok","timestamp":1748518726289,"user_tz":-120,"elapsed":281,"user":{"displayName":"AAEA","userId":"14660609484008760889"}}},"outputs":[],"source":["train_loader = DataLoader(train_ds, batch_size=16, shuffle=True,  num_workers=4, pin_memory=False)\n","val_loader = DataLoader(val_ds,   batch_size=16, shuffle=False, num_workers=4, pin_memory=False)\n","\n","model = KeypointHeatmapNet(num_keypoints=50).to(device)\n","criterion = focal_heatmap_loss\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n","                mode='min', factor=0.5, patience=5, verbose=True)\n","\n","early_stop_patience = 10"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1748518727799,"user":{"displayName":"AAEA","userId":"14660609484008760889"},"user_tz":-120},"id":"ixinxbRyYwOc","outputId":"3e9af9ef-a69d-4e35-a975-03ac00cc53f3"},"outputs":[{"output_type":"stream","name":"stdout","text":["711\n"]},{"output_type":"execute_result","data":{"text/plain":["178"]},"metadata":{},"execution_count":44}],"source":["print(len(train_loader))\n","len(val_loader)"]},{"cell_type":"markdown","metadata":{"id":"Ut_LByF-NPbt"},"source":["Smoke test for one batch (contains forward-pass, loss calculation and error check)"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":67,"status":"ok","timestamp":1748518729603,"user":{"displayName":"AAEA","userId":"14660609484008760889"},"user_tz":-120},"id":"qCSim7IKNO_2","outputId":"b856fc53-b919-4c88-a04e-d6fd0a48f755"},"outputs":[{"output_type":"stream","name":"stdout","text":["Smoke test - loss: 0.7046369910240173\n"]}],"source":["model.train()\n","preds = model(imgs.to(device))  #forward\n","loss = criterion(preds, hms.to(device))\n","print(\"Smoke test - loss:\", loss.item())"]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","# for visualizations\n","train_losses = []\n","val_losses   = []\n","\n","num_epochs = 25\n","best_val_loss = float(\"inf\")\n","\n","for epoch in range(1, num_epochs+1):\n","    model.train()\n","    running_train_loss = 0.0\n","\n","    # tqdm for training\n","    for imgs, gt_maps in tqdm(train_loader, desc=f\"Epoch {epoch} - Training\", leave=False):\n","        imgs, gt_maps = imgs.to(device), gt_maps.to(device)\n","\n","        # forward\n","        preds = model(imgs)\n","        loss  = criterion(preds, gt_maps)\n","\n","        # backward + step\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_train_loss += loss.item() * imgs.size(0)\n","\n","    epoch_train_loss = running_train_loss / len(train_loader.dataset)\n","    train_losses.append(epoch_train_loss)\n","\n","    model.eval()\n","    running_val_loss = 0.0\n","    with torch.no_grad():\n","        # tqdm for validation\n","        for imgs, gt_maps in tqdm(val_loader, desc=f\"Epoch {epoch} - Validation\", leave=False):\n","            imgs, gt_maps = imgs.to(device), gt_maps.to(device)\n","            preds = model(imgs)\n","            running_val_loss += criterion(preds, gt_maps).item() * imgs.size(0)\n","\n","    epoch_val_loss = running_val_loss / len(val_loader.dataset)\n","    val_losses.append(epoch_val_loss)\n","\n","    # early stop\n","    scheduler.step(epoch_val_loss)\n","\n","    if epoch_val_loss < best_val_loss:\n","        best_val_loss = epoch_val_loss\n","        no_improve = 0\n","        torch.save(model.state_dict(), \"best_fps_model_50_keypoints_focal_hm.pth\")\n","    else:\n","        no_improve += 1\n","\n","    if no_improve >= early_stop_patience:\n","        print(f\"Early stopping at epoch {epoch} (no improvement for {early_stop_patience} epochs)\")\n","        break\n","\n","    print(f\"Epoch {epoch:02d}/{num_epochs}  \"\n","          f\"Train Loss: {epoch_train_loss:.4f}  \"\n","          f\"Val Loss:   {epoch_val_loss:.4f}\")\n","\n","    if epoch_val_loss < best_val_loss:\n","        best_val_loss = epoch_val_loss\n","        torch.save(model.state_dict(), \"best_fps_model_50_keypoints_focal_hm.pth\")\n","\n","    print(f\"Finished Epoch {epoch}\")\n","\n","print(\"Training finished!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j-fumrmzdWat","outputId":"ab08af71-9598-4563-c265-c192b748fe2a","executionInfo":{"status":"ok","timestamp":1748527278901,"user_tz":-120,"elapsed":8537307,"user":{"displayName":"AAEA","userId":"14660609484008760889"}}},"execution_count":46,"outputs":[{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 01/25  Train Loss: 0.1479  Val Loss:   0.0609\n","Finished Epoch 1\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 02/25  Train Loss: 0.0415  Val Loss:   0.0280\n","Finished Epoch 2\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 03/25  Train Loss: 0.0227  Val Loss:   0.0204\n","Finished Epoch 3\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 04/25  Train Loss: 0.0175  Val Loss:   0.0175\n","Finished Epoch 4\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 05/25  Train Loss: 0.0150  Val Loss:   0.0157\n","Finished Epoch 5\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 06/25  Train Loss: 0.0135  Val Loss:   0.0146\n","Finished Epoch 6\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 07/25  Train Loss: 0.0125  Val Loss:   0.0138\n","Finished Epoch 7\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 08/25  Train Loss: 0.0118  Val Loss:   0.0133\n","Finished Epoch 8\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 09/25  Train Loss: 0.0113  Val Loss:   0.0132\n","Finished Epoch 9\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 10/25  Train Loss: 0.0110  Val Loss:   0.0127\n","Finished Epoch 10\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 11/25  Train Loss: 0.0108  Val Loss:   0.0126\n","Finished Epoch 11\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 12/25  Train Loss: 0.0107  Val Loss:   0.0125\n","Finished Epoch 12\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 13/25  Train Loss: 0.0106  Val Loss:   0.0123\n","Finished Epoch 13\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 14/25  Train Loss: 0.0105  Val Loss:   0.0123\n","Finished Epoch 14\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 15/25  Train Loss: 0.0104  Val Loss:   0.0123\n","Finished Epoch 15\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 16/25  Train Loss: 0.0104  Val Loss:   0.0123\n","Finished Epoch 16\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 17/25  Train Loss: 0.0103  Val Loss:   0.0123\n","Finished Epoch 17\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 18/25  Train Loss: 0.0102  Val Loss:   0.0121\n","Finished Epoch 18\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 19/25  Train Loss: 0.0102  Val Loss:   0.0121\n","Finished Epoch 19\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 20/25  Train Loss: 0.0102  Val Loss:   0.0123\n","Finished Epoch 20\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 21/25  Train Loss: 0.0101  Val Loss:   0.0121\n","Finished Epoch 21\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 22/25  Train Loss: 0.0101  Val Loss:   0.0123\n","Finished Epoch 22\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 23/25  Train Loss: 0.0101  Val Loss:   0.0120\n","Finished Epoch 23\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 24/25  Train Loss: 0.0100  Val Loss:   0.0121\n","Finished Epoch 24\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 25/25  Train Loss: 0.0100  Val Loss:   0.0120\n","Finished Epoch 25\n","Training finished!\n"]}]},{"cell_type":"markdown","metadata":{"id":"JOHTpBlFY6up"},"source":["Save the model"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"v8P0PmGkY6S7","executionInfo":{"status":"ok","timestamp":1748527296567,"user_tz":-120,"elapsed":323,"user":{"displayName":"AAEA","userId":"14660609484008760889"}}},"outputs":[],"source":["import torch\n","SAVE_PATH = \"/content/drive/MyDrive/MLDL/6D-Pose-Estimation/models/resnet/resnet18_fps_model_50_keypoints_focalhm.pth\"\n","torch.save(model.state_dict(), SAVE_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6kfu6tFwRNkn"},"outputs":[],"source":["# #for visualizations\n","# train_losses = []\n","# val_losses   = []\n","\n","# num_epochs = 25\n","# best_val_loss = float(\"inf\")\n","\n","# for epoch in range(1, num_epochs+1):\n","#     model.train()\n","#     running_train_loss = 0.0\n","\n","#     for imgs, gt_maps in train_loader:\n","#         imgs, gt_maps = imgs.to(device), gt_maps.to(device)\n","\n","#         # forward\n","#         preds = model(imgs)\n","#         loss  = criterion(preds, gt_maps)\n","\n","#         # backward + step\n","#         optimizer.zero_grad()\n","#         loss.backward()\n","#         optimizer.step()\n","\n","#         running_train_loss += loss.item() * imgs.size(0)\n","\n","#     epoch_train_loss = running_train_loss / len(train_loader.dataset)\n","#     train_losses.append(epoch_train_loss)\n","\n","#     model.eval()\n","#     running_val_loss = 0.0\n","#     with torch.no_grad():\n","#         for imgs, gt_maps in val_loader:\n","#             imgs, gt_maps = imgs.to(device), gt_maps.to(device)\n","#             preds = model(imgs)\n","#             running_val_loss += criterion(preds, gt_maps).item() * imgs.size(0)\n","\n","#     epoch_val_loss = running_val_loss / len(val_loader.dataset)\n","#     val_losses.append(epoch_val_loss)\n","\n","#     #early stop\n","#     scheduler.step(epoch_val_loss)\n","\n","#     if epoch_val_loss < best_val_loss:\n","#         best_val_loss = epoch_val_loss\n","#         no_improve = 0\n","#         torch.save(model.state_dict(), \"best_cps_model_50_keypoints_focal_hm.pth\")\n","#     else:\n","#         no_improve += 1\n","\n","#     if no_improve >= early_stop_patience:\n","#         print(f\"Early stopping at epoch {epoch} (no improvement for {early_stop_patience} epochs)\")\n","#         break\n","\n","#     print(f\"Epoch {epoch:02d}/{num_epochs}  \"\n","#           f\"Train Loss: {epoch_train_loss:.4f}  \"\n","#           f\"Val Loss:   {epoch_val_loss:.4f}\")\n","\n","#     if epoch_val_loss < best_val_loss:\n","#         best_val_loss = epoch_val_loss\n","#         torch.save(model.state_dict(), \"best_cps_model_50_keypoints_focal_hm.pth\")\n","\n","#     print(f\"Finished Epoch {epoch}\")\n","\n","# print(\"Training finished!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O7H8LCpBOiNv"},"outputs":[],"source":["# # --- 1) Plot train i val loss-a ---\n","# epochs = range(1, len(train_losses) + 1)\n","\n","# plt.figure(figsize=(8,5))\n","# plt.plot(epochs, train_losses, label='Train Loss')\n","# plt.plot(epochs, val_losses,   label='Val Loss')\n","# plt.xlabel('Epoch')\n","# plt.ylabel('Loss')\n","# plt.title('Training vs. Validation Loss')\n","# plt.legend()\n","# plt.grid(True)\n","# plt.show()\n","\n","# model.eval()\n","\n","# mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n","# std  = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n","\n","# imgs, gt_maps = next(iter(val_loader))\n","# imgs, gt_maps = imgs[:3], gt_maps[:3]\n","\n","# with torch.no_grad():\n","#     preds = model(imgs.to(device)).cpu()\n","\n","# imgs = imgs.cpu()\n","\n","import numpy as np\n","\n","def get_keypoints_from_heatmaps(hm_np):\n","    \"\"\"\n","    hm_np: numpy array with shape [K, H, W]\n","    Returns array of shape [K, 2] with (x64, y64).\n","    \"\"\"\n","    coords = []\n","    K, H, W = hm_np.shape\n","    for i in range(K):\n","        flat_idx = np.argmax(hm_np[i])\n","        y, x = divmod(flat_idx, W)\n","        coords.append((x, y))\n","    return np.array(coords)\n","\n","scale = 4  # 64 → 256\n","\n","# За сваког од првих 3 примера\n","for i in range(3):\n","    # Denormalize и конвертуј у H×W×C за plt\n","    img = imgs[i] * std + mean\n","    img = img.permute(1,2,0).numpy()\n","\n","    # Припреми GT и предикцију у numpy формату\n","    gt_hm_np   = gt_maps[i].numpy()      # [50, 64, 64]\n","    pred_hm_np = preds[i].numpy()        # [50, 64, 64]\n","\n","    # Извези 64×64 координате\n","    true_pts_64 = get_keypoints_from_heatmaps(gt_hm_np)\n","    pred_pts_64 = get_keypoints_from_heatmaps(pred_hm_np)\n","\n","    # Upscale на 256×256\n","    true_pts_256 = true_pts_64 * scale\n","    pred_pts_256 = pred_pts_64 * scale\n","\n","    # Plot\n","    plt.figure(figsize=(4,4))\n","    plt.imshow(img)\n","    plt.scatter(true_pts_256[:, 0], true_pts_256[:, 1],\n","                c='lime', marker='o', label='GT')\n","    plt.scatter(pred_pts_256[:, 0], pred_pts_256[:, 1],\n","                c='red', marker='x',  label='Pred')\n","    plt.legend(loc='lower right')\n","    plt.axis('off')\n","    plt.show()\n"]},{"cell_type":"code","source":["for hm in preds[0]:   # preds[0] has shape [50,64,64], iterating gives 50 arrays 64×64\n","    plt.imshow(hm.detach().numpy(), cmap='viridis')\n","    plt.show()"],"metadata":{"id":"Td7FsAfBy-aH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# preds.shape == (B, K, H, W)\n","batch_idx   = 1   # prva slika u batchu\n","keypoint_id = 17   # koju mapu gledaš\n","\n","single_hm = preds[batch_idx, keypoint_id].detach().numpy()  # shape (64,64)\n","plt.figure(figsize=(4,4))\n","plt.imshow(single_hm, cmap='viridis')\n","plt.title(f\"Image {batch_idx}, Keypoint {keypoint_id}\")\n","plt.colorbar()\n","plt.show()"],"metadata":{"id":"amdsoeFS1NQV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zwv6H0caJT6O"},"source":["# Defining the JSON file with all keypoints"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"DzhmTtMuJfyj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748527367651,"user_tz":-120,"elapsed":40072,"user":{"displayName":"AAEA","userId":"14660609484008760889"}},"outputId":"b5918c23-544d-4317-d9d7-a8190a2db03a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Saved 2843 entries to /content/drive/MyDrive/MLDL/6D-Pose-Estimation/data/predicted_key_points/2D_predicted_resnet18_keypoints50_fps_focalloss.json\n"]}],"source":["def get_all_keypoints(preds, hmap_size=64, img_size=256):\n","    B, K, H, W = preds.shape\n","    scale = img_size / hmap_size\n","    offset = scale / 2\n","    preds = preds.cpu()\n","    all_results = []\n","\n","    for b in range(B):\n","        heatmaps = preds[b]  # [K, H, W]\n","        pts = []\n","        for idx in range(K):\n","            hm = heatmaps[idx]\n","            flat = hm.view(-1)\n","            pos = flat.argmax().item()\n","            y, x = divmod(pos, W)\n","            x = x * scale + offset\n","            y = y * scale + offset\n","            # conf = float(hm.view(-1).max())\n","            pts.append([x, y])\n","        all_results.append(pts)\n","\n","    return all_results\n","\n","subset = val_loader.dataset\n","orig_ds = subset.dataset\n","indices = subset.indices\n","batch_size = val_loader.batch_size\n","\n","predictions = {}\n","\n","for batch_i, (imgs, _) in enumerate(val_loader):\n","    with torch.no_grad():\n","        preds = model(imgs.to(device)).cpu()\n","\n","    batch_keypoints = get_all_keypoints(preds)\n","    for i, pts in enumerate(batch_keypoints):\n","        orig_idx = indices[batch_i * batch_size + i]\n","        img_id = orig_ds.basenames[orig_idx]\n","        predictions[img_id] = pts\n","\n","out_dir = \"/content/drive/MyDrive/MLDL/6D-Pose-Estimation/data/predicted_key_points\"\n","os.makedirs(out_dir, exist_ok=True)\n","out_path = os.path.join(out_dir, \"2D_predicted_resnet18_keypoints50_fps_focalloss.json\")\n","\n","with open(out_path, \"w\") as f:\n","    json.dump(predictions, f, indent=2, sort_keys=True)\n","\n","print(f\"Saved {len(predictions)} entries to {out_path}\")"]},{"cell_type":"markdown","metadata":{"id":"i5romJUXkynR"},"source":["# Junk methods"]},{"cell_type":"markdown","metadata":{"id":"b15q9JhNNHVP"},"source":["## Choose the best 4 key points"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HsDgqkY4NFrU"},"outputs":[],"source":["def select_topk_keypoints_from_heatmaps(preds, topk=4, hmap_size=64, img_size=256):\n","\n","    B, K, H, W = preds.shape\n","    assert H == W == hmap_size\n","    scale  = img_size / hmap_size\n","    offset = scale / 2\n","\n","    preds_cpu = preds.cpu()\n","    results = []\n","\n","    for i in range(B):\n","        hm = preds_cpu[i]  # Tensor[K, H, W]\n","\n","        flat_confs = hm.view(K, -1)\n","        confs = flat_confs.max(dim=1).values\n","\n","        topk_vals, topk_idx = torch.topk(confs, topk, largest=True)\n","\n","        coords = []\n","        for idx in topk_idx:\n","            channel_map = hm[idx]\n","            flat_map = channel_map.view(-1)\n","            max_pos = flat_map.argmax().item()\n","            y, x = divmod(max_pos, W)\n","\n","            x_img = x * scale + offset\n","            y_img = y * scale + offset\n","            coords.append((x_img, y_img))\n","\n","        results.append({\n","            'indices': topk_idx.tolist(),\n","            'scores':  topk_vals.tolist(),\n","            'coords2d': coords\n","        })\n","\n","    return results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Aatc0RQPNFME"},"outputs":[],"source":["import torch\n","import json\n","\n","def select_topk_keypoints_from_heatmaps(preds, topk=4, hmap_size=64, img_size=256):\n","    B, K, H, W = preds.shape\n","    assert H == W == hmap_size\n","    scale  = img_size / hmap_size\n","    offset = scale / 2\n","\n","    preds_cpu = preds.cpu()\n","    results = []\n","\n","    for i in range(B):\n","        hm = preds_cpu[i]  # Tensor[K, H, W]\n","\n","        flat_confs = hm.view(K, -1)\n","        confs = flat_confs.max(dim=1).values\n","\n","        topk_vals, topk_idx = torch.topk(confs, topk, largest=True)\n","\n","        coords = []\n","        for idx in topk_idx:\n","            channel_map = hm[idx]\n","            flat_map = channel_map.view(-1)\n","            max_pos = flat_map.argmax().item()\n","            y, x = divmod(max_pos, W)\n","\n","            x_img = x * scale + offset\n","            y_img = y * scale + offset\n","            coords.append((x_img, y_img))\n","\n","        results.append({\n","            'indices': topk_idx.tolist(),\n","            'scores':  topk_vals.tolist(),\n","            'coords2d': coords\n","        })\n","\n","    return results\n","\n","# ——————————————————————————————————————————————\n","#  Load your trained model\n","model.load_state_dict(torch.load(\"best_model.pth\", map_location=device))\n","model.to(device)\n","model.eval()\n","\n","#  Prepare for saving predictions\n","predicted = {}\n","\n","#  val_loader.dataset is a Subset wrapping your KeypointHeatmapDataset\n","subset: torch.utils.data.Subset = val_loader.dataset\n","orig_ds       = subset.dataset        # the original KeypointHeatmapDataset\n","subset_indices = subset.indices       # maps subset-pos → original-pos\n","batch_size     = val_loader.batch_size\n","\n","#  Run through validation set and collect top-4 keypoints\n","for batch_idx, (imgs, _) in enumerate(val_loader):\n","    imgs = imgs.to(device)\n","    with torch.no_grad():\n","        preds = model(imgs).cpu()\n","\n","    best4 = select_topk_keypoints_from_heatmaps(preds, topk=4)\n","\n","    for i, item in enumerate(best4):\n","        # extract the 2D coords\n","        coords2d = item[\"coords2d\"]\n","\n","        # map subset position → original index → basename\n","        subset_pos = batch_idx * batch_size + i\n","        orig_idx   = subset_indices[subset_pos]\n","        img_id     = orig_ds.basenames[orig_idx]\n","\n","        predicted[img_id] = coords2d\n","\n","#  Write out to JSON\n","out_path = \"/content/drive/MyDrive/MLDL/6D-Pose-Estimation/data/predicted_key_points/2D_predicted_key_points_fps.json\"\n","with open(out_path, \"w\") as f:\n","    json.dump(predicted, f, indent=2)\n","\n","print(f\"Saved {len(predicted)} entries to {out_path}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sV0o5NPTVRTs"},"outputs":[],"source":["def keypoints_from_heatmaps(preds, hmap_size=64, img_size=256):\n","    B, K, H, W = preds.shape\n","    assert H == W == hmap_size\n","    scale  = img_size / hmap_size\n","    offset = scale / 2\n","\n","    preds_cpu = preds.cpu()\n","    results = []\n","    #for each batch\n","    for i in range(B):\n","        hm = preds_cpu[i]  # Tensor[K, H, W]\n","\n","        coords = []\n","        confs  = []\n","\n","        for idx in range(K):\n","          channel_map = hm[idx]\n","          flat_map = channel_map.view(-1)\n","          max_pos = flat_map.argmax().item()\n","          confs.append(flat_map[max_pos].item())\n","\n","          y, x = divmod(max_pos, W)\n","          x_img = x * scale + offset\n","          y_img = y * scale + offset\n","          coords.append((x_img, y_img))\n","\n","        results.append({\n","            'indices': list(range(K)),\n","            'scores':  confs,\n","            'coords2d': coords\n","        })\n","\n","    return results\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VYpzKb8BQELs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748194622142,"user_tz":-120,"elapsed":669618,"user":{"displayName":"AAEA","userId":"14660609484008760889"}},"outputId":"90e62b36-8d16-474b-9517-8c848644df79"},"outputs":[{"output_type":"stream","name":"stdout","text":["Saved 2843 entries to /content/drive/MyDrive/MLDL/6D-Pose-Estimation/data/predicted_key_points/2D_predicted_key_points_fps_50.json\n"]}],"source":["# ——————————————————————————————————————————————\n","#  Load your trained model\n","model = KeypointHeatmapNet(num_keypoints=50).to(device)\n","model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n","# model.to(device)\n","model.eval()\n","\n","#  Prepare for saving predictions\n","predicted = {}\n","\n","#  val_loader.dataset is a Subset wrapping your KeypointHeatmapDataset\n","subset: torch.utils.data.Subset = val_loader.dataset\n","orig_ds       = subset.dataset        # the original KeypointHeatmapDataset\n","subset_indices = subset.indices       # maps subset-pos → original-pos\n","batch_size     = val_loader.batch_size\n","\n","#  Run through validation set and collect top-4 keypoints\n","for batch_idx, (imgs, _) in enumerate(val_loader):\n","    imgs = imgs.to(device)\n","    with torch.no_grad():\n","        preds = model(imgs).cpu()\n","\n","    keypoints = keypoints_from_heatmaps(preds)\n","\n","    for i, item in enumerate(keypoints):\n","        # extract the 2D coords\n","        coords_con2d = item[\"coords2d\"] # DODAJ OVDE ZA CONFIDENCE\n","\n","        # map subset position → original index → basename\n","        subset_pos = batch_idx * batch_size + i\n","        orig_idx   = subset_indices[subset_pos]\n","        img_id     = orig_ds.basenames[orig_idx]\n","\n","        predicted[img_id] = coords_con2d\n","\n","#  Write out to JSON\n","out_path = \"/content/drive/MyDrive/MLDL/6D-Pose-Estimation/data/predicted_key_points/2D_predicted_key_points_fps_50.json\"\n","with open(out_path, \"w\") as f:\n","    json.dump(predicted, f, indent=2)\n","\n","print(f\"Saved {len(predicted)} entries to {out_path}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p7Eh37_cYxjZ"},"outputs":[],"source":["SAVE_PATH = \"/content/drive/MyDrive/MLDL/6D-Pose-Estimation/best_model.pth\"\n","torch.save(model.state_dict(), SAVE_PATH)\n"]},{"cell_type":"markdown","metadata":{"id":"MD4hwI30IMea"},"source":["# After training"]},{"cell_type":"code","source":["checkpoint_path = \"/content/drive/MyDrive/MLDL/6D-Pose-Estimation/models/resnet/resnet18_cps_model_50_keypoints_focalhm.pth\"\n"],"metadata":{"id":"auR_dwe6ePqM"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"9cCKHaw9IPBH"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = KeypointHeatmapNet(num_keypoints=50).to(device)\n","checkpoint_path = \"/content/drive/MyDrive/MLDL/6D-Pose-Estimation/models/resnet/resnet18_cps_model_50_keypoints_focalhm.pth\"\n","state_dict = torch.load(checkpoint_path, map_location=device)\n","model.load_state_dict(state_dict)\n","model.eval()"]}],"metadata":{"colab":{"collapsed_sections":["i5romJUXkynR"],"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}